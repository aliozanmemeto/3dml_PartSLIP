{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23bc67d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../GroundingDINO')\n",
    "sys.path.insert(0,'../SAM')\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from pytorch3d.io import IO\n",
    "import numpy as np\n",
    "from src.utils import normalize_pc,save_colored_pc\n",
    "from src.render_pc import render_pc\n",
    "from src.gen_superpoint import gen_superpoint\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "from torchvision.ops import nms\n",
    "import json\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import distinctipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d54d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolobbox2bbox(yolobox):\n",
    "    x = yolobox[:,0]\n",
    "    y = yolobox[:,1]\n",
    "    w = yolobox[:,2]\n",
    "    h = yolobox[:,3]\n",
    "    xyxy = np.zeros_like(yolobox)\n",
    "    xyxy[:,0] = x-w/2\n",
    "    xyxy[:,1] = y-h/2\n",
    "    xyxy[:,2] = x+w/2\n",
    "    xyxy[:,3] = y+h/2\n",
    "    return xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88597d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pc_within_bbox(x1, y1, x2, y2, pc):  \n",
    "    flag = np.logical_and(pc[:, 0] > x1, pc[:, 0] < x2)\n",
    "    flag = np.logical_and(flag, pc[:, 1] > y1)\n",
    "    flag = np.logical_and(flag, pc[:, 1] < y2)\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1786d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDinoPrompt(metaData,className):\n",
    "    listOfParts = metaData[className]\n",
    "    prompt = \"\"\n",
    "    partList = {}\n",
    "    for i,part in enumerate(listOfParts):\n",
    "        prompt += f\"{className} {part}.\".lower()\n",
    "        partList[f\"{className} {part}\".lower()] = i\n",
    "    return prompt,partList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca9ca65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InferDINOSAMZeroShot(input_pc_file, category, modelDINO, predictorSAM, metaData, device,BOX_TRESHOLD = 0.2,\n",
    "    TEXT_TRESHOLD = 0.3, SCORE_THRESHOLD=0.2, n_neighbors = 21, n_pass=3, save_dir=\"tmp\"):\n",
    "    \n",
    "#     print(\"-----Zero-shot inference of %s-----\" % input_pc_file)\n",
    "    TEXT_PROMPT,partList = toDinoPrompt(metaData, category)\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{save_dir}/rendered_img\", exist_ok=True) #create the necessary save directories\n",
    "    os.makedirs(f\"{save_dir}/dino_pred\", exist_ok=True)\n",
    "    os.makedirs(f\"{save_dir}/semantic_segDino_KNN\", exist_ok=True)\n",
    "    \n",
    "    io = IO()\n",
    "    xyz, rgb = normalize_pc(input_pc_file, save_dir, io, device) #read Point cloud and rgb in the format n,3\n",
    "    img_dir, pc_idx, screen_coords = render_pc(xyz, rgb, save_dir, device) #create the rendered 2D images and return \n",
    "    # pc_idx = hxw where every pixel has a PC index correspondence   \n",
    "    preds = []\n",
    "    for i in range(pc_idx.shape[0]):\n",
    "        image_source, image = load_image(f\"{save_dir}/rendered_img/{i}.png\") #load rgb images\n",
    "        predictorSAM.set_image(image_source)\n",
    "#         print(\"[dino inference...]\")\n",
    "        boxes, logits, phrases = predict(\n",
    "                                        model=modelDINO,\n",
    "                                        image=image,\n",
    "                                        caption=TEXT_PROMPT,\n",
    "                                        box_threshold=BOX_TRESHOLD,\n",
    "                                        text_threshold=TEXT_TRESHOLD\n",
    "                                    )\n",
    "        phrases = np.array(phrases) #just to fix indexing\n",
    "\n",
    "        xyxy = yolobbox2bbox(boxes)*image.shape[-1] #change bbox format to xyxy and scale with image size\n",
    "        \n",
    "        nms_mask = []\n",
    "        for t,bbox in enumerate(xyxy): \n",
    "            if check_pc_within_bbox(bbox[0], bbox[1], bbox[2], bbox[3], screen_coords[i]).mean() < 0.95: \n",
    "                nms_mask.append(t)\n",
    "        xyxy = xyxy[nms_mask]\n",
    "        boxes = boxes[nms_mask]\n",
    "        logits = logits[nms_mask]\n",
    "        phrases = phrases[nms_mask]\n",
    "        \n",
    "        \n",
    "        \n",
    "        nms_indexes = nms(torch.tensor(xyxy) , logits, 0.5).numpy() #non maximum supression\n",
    "\n",
    "        nms_mask = []\n",
    "        for t,index in enumerate(nms_indexes):\n",
    "            if phrases[index].lower() in partList.keys():\n",
    "                nms_mask.append(t)\n",
    "        nms_indexes = nms_indexes[nms_mask] #this is a temporary fix for DINO returning different classes that are not in the PROMPT\n",
    "        # another fix is needed for this as this eleminates some important segments such as chair back as the phrase is not exact to PROMPT\n",
    "\n",
    "        input_boxes = torch.tensor(xyxy[nms_indexes], device=predictorSAM.device)    \n",
    "        transformed_boxes = predictorSAM.transform.apply_boxes_torch(input_boxes, image_source.shape[:2])\n",
    "\n",
    "        if(transformed_boxes.numel() == 0):\n",
    "            transformed_boxes = None\n",
    "            \n",
    "        masks, _, _ = predictorSAM.predict_torch(\n",
    "            point_coords=None,\n",
    "            point_labels=None,\n",
    "            boxes=transformed_boxes,\n",
    "            multimask_output=False,\n",
    "        )    #create segmentation masks with sam\n",
    "\n",
    "        for index,j in enumerate(nms_indexes):\n",
    "            preds.append({'image_id': i, 'category_id': phrases[j], \n",
    "                          'bbox': boxes[j]*image.shape[-1], \n",
    "                          'score': logits[j],\n",
    "                          'mask':masks[index,0]   \n",
    "                         }\n",
    "                        )\n",
    "        annotated_frame = annotate(image_source=image_source, boxes=boxes[nms_indexes], logits=logits[nms_indexes], phrases=phrases[nms_indexes])\n",
    "        cv2.imwrite(f\"{save_dir}/dino_pred/{i}.png\", annotated_frame) #save an annotated image for DINO debugging\n",
    "        \n",
    "    pc_aggMask = torch.zeros((xyz.shape[0],len(partList)+1)) #this is a segment agg mask we sum all the scores from our bboxes \n",
    "    #into their own respective channel, the last channel is for unsegmented parts\n",
    "    pc_aggMask[:,-1] = SCORE_THRESHOLD #we can set a confidence threshold by setting the unsegmented score\n",
    "    for prediction in preds:\n",
    "        maskedPC_idx = pc_idx[prediction[\"image_id\"],prediction[\"mask\"].cpu().numpy()] #this gives you the pc idx of the points that are inside the mask\n",
    "        index_pcMasked = np.unique(maskedPC_idx)[1:] # we only need the unique idx and the first id is always -1 meaning not found\n",
    "        pc_aggMask[index_pcMasked,partList[prediction[\"category_id\"]]] += prediction[\"score\"] #add up all the scores for each part\n",
    "    pc_seg_classes = torch.argmax(pc_aggMask,dim=-1) #select the highest score as our segmentation class\n",
    "    #if non of the part scores are over the SCORE_THRESHOLD it will be left unsegmented\n",
    "    partColors = distinctipy.get_colors(len(partList))\n",
    "    rgb_sem_merged = np.zeros((xyz.shape[0], 3))\n",
    "    # since projections are not exact meaning not every PC point is rendered into our image our backprojections are not dense\n",
    "    # use KNN to smooth these backprojections \n",
    "    nn = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree').fit(xyz) #create a knn\n",
    "    \n",
    "    results = {\"partseg_rgbs\":{}}\n",
    "    for colorId,part in enumerate(partList):\n",
    "        pc_part_idx = np.zeros((xyz.shape[0]),dtype=int)\n",
    "        rgb_sem = np.zeros((xyz.shape[0],3))\n",
    "        pc_part_idx[torch.where(pc_seg_classes==partList[part])] = 1\n",
    "        \n",
    "        for pass_ in range(n_pass):\n",
    "            notColoredIndexes = torch.where(pc_seg_classes!=partList[part]) #find non segmented parts for smoothing\n",
    "\n",
    "            n_indexes = nn.kneighbors(xyz[notColoredIndexes],n_neighbors+1,return_distance=False)\n",
    "            n_indexes = n_indexes[:,1:] #get n_neighbors for the points, the first index is always the point itself so delete that\n",
    "            #we have dense point clouds so distance based measures are not necessary and sometimes give worst results\n",
    "            flag = pc_part_idx[n_indexes].mean(axis=1) \n",
    "            \n",
    "            flag[np.where(flag>=0.4)] = 1 #and segmnent the points where the mean of neighbours are colored %40 or over\n",
    "            flag[np.where(flag<0.4)] = 0\n",
    "            pc_part_idx[notColoredIndexes] = flag\n",
    "           \n",
    "        rgb_sem[pc_part_idx.astype(bool)] = partColors[colorId]\n",
    "        rgb_sem_merged += rgb_sem\n",
    "        save_colored_pc(f\"{save_dir}/semantic_segDino_KNN/{part}.ply\", xyz, rgb_sem)\n",
    "    \n",
    "        results[\"partseg_rgbs\"][part] = rgb_sem\n",
    "        \n",
    "    save_colored_pc(f\"{save_dir}/semantic_segDino_KNN/{category}.ply\", xyz, rgb_sem_merged)\n",
    "    results[\"partList\"] = partList\n",
    "    results[\"xyz\"] = xyz\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f89a8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a157ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "modelDINO = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\",\n",
    "                       \"../GroundingDINO/weights/groundingdino_swinb_cogcoor.pth\",\n",
    "                      device=device\n",
    "                      )\n",
    "\n",
    "sam_checkpoint = \"../SAM/weights/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=\"cuda\")\n",
    "\n",
    "predictorSAM = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d04dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaData = json.load(open(\"./PartNetE_meta.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "601da3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Suitcase\"\n",
    "input_pc_file = f\"data/{category}//.ply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b37a83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = \"Chair\"\n",
    "input_pc_file = f\"data/{category}/3091\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0c22bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An exception occurred in telemetry logging.Disabling telemetry to prevent further exceptions.\n",
      "Traceback (most recent call last):\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/file_io.py\", line 946, in __log_tmetry_keys\n",
      "    handler.log_event()\n",
      "  File \"/rhome/kamburoglu/miniconda3/envs/partslip/lib/python3.9/site-packages/iopath/common/event_logger.py\", line 97, in log_event\n",
      "    del self._evt\n",
      "AttributeError: _evt\n",
      "UserWarning: R is not a valid rotation matrix\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    }
   ],
   "source": [
    "preds = InferDINOSAMZeroShot(input_pc_file + \"/pc.ply\", category, modelDINO, predictorSAM, metaData, device, BOX_TRESHOLD = 0.2,\n",
    "    TEXT_TRESHOLD = 0.3, SCORE_THRESHOLD=0.2, n_neighbors = 21, n_pass=5, save_dir=f'examples/zeroshot_{category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c534a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
